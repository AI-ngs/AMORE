from collections import Counter
from sudachipy import dictionary, tokenizer
import re

# ===============================
# í˜•íƒœì†Œ ë¶„ì„ê¸°
# ===============================
tokenizer_obj = dictionary.Dictionary().create()
mode = tokenizer.Tokenizer.SplitMode.C

# ===============================
# ê°ì„± ë‹¨ì–´ (ì´í‰ìš©)
# ===============================
POSITIVE_WORDS = ["è‰¯ã„", "å¥½ã", "æº€è¶³", "ãŠã™ã™ã‚", "è‰¯ã‹ã£ãŸ"]
NEGATIVE_WORDS = ["æ‚ªã„", "ä¸æº€", "å¾®å¦™", "åˆã‚ãªã„"]

# ===============================
# ğŸ”¥ ê°•í™”ëœ ë¶ˆìš©ì–´
# ===============================
STOPWORDS = set([
    "ã™ã‚‹", "ã‚ã‚‹", "ã„ã‚‹", "ãªã‚‹", "æ€ã†",
    "ã“ã‚Œ", "ãã‚Œ", "ãŸã‚", "ã¨ã“ã‚", "ã‚ˆã†",
    "æ„Ÿã˜", "æ–¹", "å•†å“", "ä½¿ç”¨", "è³¼å…¥",
    "ä»Šå›", "ä»–", "è‡ªåˆ†", "ç§", "ã‚‚ã®", "å ´åˆ",
    "ç¾å“", "å ´æ‰€", "åŠ¹æœ", "é–¢é€£", "ãƒ¯ãƒ¼ãƒ‰",
    "è¨˜äº‹", "ç´¹ä»‹", "å†…å®¹", "æƒ…å ±", "å†™çœŸ",
    "ãƒšãƒ¼ã‚¸", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "æŠ•ç¨¿", "è©•ä¾¡",
    "å…¨ä½“", "å°è±¡", "æ„å‘³", "ç†ç”±", "çµæœ",
    "ã¨ã¦ã‚‚", "ã‹ãªã‚Š", "å°‘ã—", "ã¡ã‚‡ã£ã¨"
])

# ===============================
# ì†ì„± + ëŒ€í‘œ í‚¤ì›Œë“œ
# ===============================
ASPECT_ANCHORS = {
    "ë³´ìŠµë ¥": ["ä¿æ¹¿"],
    "ë°œë¦¼ì„±": ["ä¼¸ã³"],
    "ì§€ì†ë ¥": ["æŒç¶š"],
    "í–¥": ["é¦™ã‚Š"],
    "ìƒ‰": ["è‰²"],
}

# ===============================
# ë¬¸ì¥ ë¶„ë¦¬ í•¨ìˆ˜ (ì¼ë³¸ì–´ ê¸°ì¤€)
# ===============================
def split_sentences(text):
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", text)
    return [s.strip() for s in sentences if len(s.strip()) > 0]

# ===============================
# ë¦¬ë·° ê°ì„± íŒë³„ (ì´í‰ìš©)
# ===============================
def get_review_sentiment(text):
    score = 0
    for p in POSITIVE_WORDS:
        if p in text:
            score += 1
    for n in NEGATIVE_WORDS:
        if n in text:
            score -= 1

    if score > 0:
        return "positive"
    elif score < 0:
        return "negative"
    return "neutral"

# ===============================
# ğŸ”¥ ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
# ===============================
def analyze_reviews(texts, top_n=5):
    # ---------------------------
    # 1ï¸âƒ£ ì´í‰ ê°ì„± ë¹„ìœ¨
    # ---------------------------
    sentiment_cnt = {"positive": 0, "neutral": 0, "negative": 0}

    for text in texts:
        sentiment_cnt[get_review_sentiment(text)] += 1

    total = sum(sentiment_cnt.values()) or 1

    sentiment_ratio = {
        "ê¸ì •": round(sentiment_cnt["positive"] / total * 100, 1),
        "ì¤‘ë¦½": round(sentiment_cnt["neutral"] / total * 100, 1),
        "ë¶€ì •": round(sentiment_cnt["negative"] / total * 100, 1),
    }

    # ---------------------------
    # 2ï¸âƒ£ ë¬¸ì¥ ë‹¨ìœ„ ì†ì„± ë¶„ì„
    # ---------------------------
    aspect_analysis = {}

    for aspect, anchors in ASPECT_ANCHORS.items():
        mention_count = 0
        word_counter = Counter()

        for text in texts:
            sentences = split_sentences(text)

            for sentence in sentences:
                if not any(anchor in sentence for anchor in anchors):
                    continue

                mention_count += 1

                for token in tokenizer_obj.tokenize(sentence, mode):
                    pos = token.part_of_speech()[0]
                    base = token.dictionary_form()

                    if pos not in ["åè©", "å½¢å®¹è©"]:
                        continue
                    if base in STOPWORDS:
                        continue
                    if len(base) <= 1:
                        continue

                    word_counter[base] += 1

        if mention_count > 0:
            aspect_analysis[aspect] = {
                "ì–¸ê¸‰_ê±´ìˆ˜": mention_count,
                "í™•ì¥_í‘œí˜„_TOP": word_counter.most_common(top_n)
            }

    # ---------------------------
    # 3ï¸âƒ£ íŒŒì´í”„ë¼ì¸ í˜¸í™˜ ë°˜í™˜
    # ---------------------------
    return {
        "positive_keywords": {},
        "negative_keywords": {},
        "sentiment_ratio": sentiment_ratio,
        "aspect_analysis": aspect_analysis
    }
